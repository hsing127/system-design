1. Scaling app to a Million usersr

Problem Statement - Build a system that supports a single user and gradually scale it up to serve millions of users.

Steps:
-- Basic Single Server setup -- 
user --> browser --> dns --> gets IP --> calls the server --> returns JSON or html page --> browser shows to the user

-- Databases 
1. Most commonly used - RDBMS (Relational Database Management Sytstem) - can do joins between tables
2. Non-relational Database systems - 4 categories - key-values stores, graph stores, column stores, and document stores; Joins not supported
Right choice if
a. Your application needs super low latency
b. Data are unstructured or no relational data
c. Only need to serialize or deserialize data
d. Need to store a massive amount of data

-- Vertical Scaling - "Scaling up" - Has a hard limit
-- Horizontal Scaling - "Scaling out"

-- Load Balancer -Web servers are unreachable by the clients. For better security, private IPs are used for communication between the servers which is used by the load balancer.

Notes:Now we have solved the issue of failover as load balancer can handle traffic and can handle more servers being added to the server pool
Data tier right now has 1 db and does not support failover and redundancy.

-- Data Replication - done with usually master/slave relationship between databases.
A master db only supports write operations. A slave db gets the copies of the data from the master db and only supports read ops. All data modifying commands like insert, delte or update must be sent to master db. Most apps require much higher ratio of reads to writes; thus the number of slaves db is usually higher than master dbs. 
Advantages:
1. Better Performance - allow more queries to be executed in parallel - reads across slaves and writes across masters.
2. Reliability - If some dbs are destroyed, data is preserved and it is replicated across multiple locations
3. High Availability - Replicating allows website to remain online even if one db is offline.

What if one db goes down - 
1. if slave goes down and it is the only slave - read ops are redirected to the master db. After the isssue if resolved a new slave can replace the old.
2. if master goes down - a slave db is promoted to master and will handle all the writes. A new slave is spun up in place of this slave. Promoting slaves if challenging as it might be out of date - data will need to be updated using recovery scripts. Also multi masters and circular replication can help[SEE THEM]

Deisgn till now - 
1. A user gets the load balancer IP using a dns service.
2. The user connects to the load balancer using this IP.
3. HTTP request is routed to either Server 1 or Server 2.
4. A web server reads the data from a slave DB.
5. A web server routes any data-modifying ops to the master DB - write, update, delete

Notes: Now we can improve the response time using cache layer and CDN for static content(JS/CSS/Image/Video files)

-- Cache - Temporary storage area that stores the result of expensive responses or frequently accessed data in memory.
Cache Tier - Data store layer much faster than the db. Benefits - better system perf, reduce db workloads, scale cache independently.
After recieving request, web server checks the cache first, if yes sends back to the client; else queries the db, stores the response in the cache and then send it back the client - this is called read through cache.
Considerations:
1. Data is read frequently but modified infrequently - cache is stored in volatile memory so not ideal for persisting data; if cache server restarts all the data is lost.
2. Expiration Policy - not too long (data can become stale), also not too short(reload data too frequently from db).
3. Consistency - Keeping the data store and the cache in sync; inconsistency can happen because data modifying ops on the data store and the cache are not in a single operation; hard to maintain cache consistency across multiple regions.(See scaling memcache at Facebook)
4. Mitigating Failures - cache server represents a single point of failure(SPOF) - if it fails entire system fails; multiple cache servers across diff data centres are recommended - also overprovision required memory by certain percentages - provides a buffer as the memory usage increases.
5. Eviction Policy - Once cache is full, any requests to add items to cache causes existing items to ber removed; LRU is most popular, LFU or FIFO can ber used.

-- Content Delivery Network (CDN)

something something
